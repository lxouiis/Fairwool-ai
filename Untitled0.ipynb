{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lxouiis/Fairwool-ai/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwBlr6I5pmAr",
        "outputId": "dc8cbfb5-7308-4074-d5d7-a9ce933512f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kj-V6HfFueI7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vdG1D691p31O"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/dataset 2.zip\" /content/dataset.zip\n",
        "!unzip -q dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dY0nItgoqyzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527491f7-d016-41b8-9602-49d3f00d02ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['grade_B', 'grade_A', 'grade_C']\n",
            "['grade_B', 'grade_A', 'grade_C']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir('/content/dataset/train'))\n",
        "print(os.listdir('/content/dataset/val'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xZg-49SGrWuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e639f6b7-73ba-4011-9eaf-8c0d2a3fb484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset  dataset.zip  drive  __MACOSX  sample_data\n",
            "./dataset/train\n",
            "./__MACOSX/dataset/train\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "!find . -maxdepth 3 -type d -name train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1P3Db7kvr3VW"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vwiAauXrrxDx"
      },
      "outputs": [],
      "source": [
        "!rm -rf __MACOSX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h8g_OG1stCFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992b641d-33b4-45b1-da49-d3732aa59cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_deit.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_deit.py\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import timm\n",
        "\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    data_dir: str\n",
        "    output_dir: str\n",
        "    model_name: str\n",
        "    img_size: int\n",
        "    batch_size: int\n",
        "    epochs_head: int\n",
        "    epochs_ft: int\n",
        "    unfreeze_blocks: int\n",
        "    lr_head: float\n",
        "    lr_ft: float\n",
        "    weight_decay: float\n",
        "    label_smoothing: float\n",
        "    num_workers: int\n",
        "    seed: int\n",
        "    patience: int\n",
        "    grad_clip: float\n",
        "\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    # Apple Silicon (MPS)\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def build_transforms(img_size: int):\n",
        "    # Training: strong but safe augmentations (textures should survive)\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size, scale=(0.65, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=8),\n",
        "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.02),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "    # Validation / inference: standard resize->center crop (256->224 style)\n",
        "    resize_size = int(round(img_size * 256 / 224))\n",
        "    val_tf = transforms.Compose([\n",
        "        transforms.Resize(resize_size),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "\n",
        "def compute_class_weights(targets: List[int], num_classes: int) -> torch.Tensor:\n",
        "    counts = torch.zeros(num_classes, dtype=torch.long)\n",
        "    for t in targets:\n",
        "        counts[t] += 1\n",
        "    counts = counts.clamp(min=1)\n",
        "    total = counts.sum().item()\n",
        "    weights = total / (num_classes * counts.float())\n",
        "    return weights\n",
        "\n",
        "\n",
        "def freeze_all_but_head(model: nn.Module):\n",
        "    for n, p in model.named_parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze classifier head(s) and final norms for stability\n",
        "    for n, p in model.named_parameters():\n",
        "        if n.startswith(\"head\") or n.startswith(\"head_dist\") or n.startswith(\"norm\") or n.startswith(\"fc_norm\"):\n",
        "            p.requires_grad = True\n",
        "\n",
        "\n",
        "def unfreeze_last_blocks(model: nn.Module, unfreeze_blocks: int):\n",
        "    # Keep everything frozen by default, then unfreeze selected parts\n",
        "    for n, p in model.named_parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze last N transformer blocks if present\n",
        "    if hasattr(model, \"blocks\"):\n",
        "        blocks = model.blocks\n",
        "        n_blocks = len(blocks)\n",
        "        start = max(0, n_blocks - unfreeze_blocks)\n",
        "        for i in range(start, n_blocks):\n",
        "            for p in blocks[i].parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "    # Also unfreeze final norm(s) and head(s)\n",
        "    for n, p in model.named_parameters():\n",
        "        if n.startswith(\"head\") or n.startswith(\"head_dist\") or n.startswith(\"norm\") or n.startswith(\"fc_norm\"):\n",
        "            p.requires_grad = True\n",
        "\n",
        "\n",
        "def make_optimizer(model: nn.Module, lr: float, weight_decay: float, head_lr_mult: float = 5.0):\n",
        "    head_params = []\n",
        "    body_params = []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if n.startswith(\"head\") or n.startswith(\"head_dist\"):\n",
        "            head_params.append(p)\n",
        "        else:\n",
        "            body_params.append(p)\n",
        "\n",
        "    param_groups = []\n",
        "    if body_params:\n",
        "        param_groups.append({\"params\": body_params, \"lr\": lr})\n",
        "    if head_params:\n",
        "        param_groups.append({\"params\": head_params, \"lr\": lr * head_lr_mult})\n",
        "\n",
        "    opt = torch.optim.AdamW(param_groups, lr=lr, weight_decay=weight_decay)\n",
        "    return opt\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Confusion matrix + macro-F1 without sklearn (keeps deps minimal)\n",
        "    num_classes = loader.dataset.classes.__len__()\n",
        "    cm = torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(images)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.numel()\n",
        "\n",
        "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "            cm[t.long(), p.long()] += 1\n",
        "\n",
        "    acc = correct / max(1, total)\n",
        "\n",
        "    # Macro F1\n",
        "    f1s = []\n",
        "    for k in range(num_classes):\n",
        "        tp = cm[k, k].item()\n",
        "        fp = cm[:, k].sum().item() - tp\n",
        "        fn = cm[k, :].sum().item() - tp\n",
        "        prec = tp / max(1, (tp + fp))\n",
        "        rec = tp / max(1, (tp + fn))\n",
        "        f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec / (prec + rec))\n",
        "        f1s.append(f1)\n",
        "    macro_f1 = sum(f1s) / len(f1s)\n",
        "\n",
        "    return {\n",
        "        \"val_acc\": acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "    }\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    grad_clip: float,\n",
        ") -> float:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            if grad_clip > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            if grad_clip > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        n += labels.size(0)\n",
        "\n",
        "    return running_loss / max(1, n)\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data_dir\", type=str, required=True, help=\"Root folder containing train/ and val/\")\n",
        "    ap.add_argument(\"--output_dir\", type=str, required=True)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"deit_small_patch16_224.fb_in1k\")\n",
        "    ap.add_argument(\"--img_size\", type=int, default=224)\n",
        "    ap.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    ap.add_argument(\"--epochs_head\", type=int, default=4)\n",
        "    ap.add_argument(\"--epochs_ft\", type=int, default=10)\n",
        "    ap.add_argument(\"--unfreeze_blocks\", type=int, default=4, help=\"Unfreeze last N transformer blocks\")\n",
        "    ap.add_argument(\"--lr_head\", type=float, default=1e-3)\n",
        "    ap.add_argument(\"--lr_ft\", type=float, default=1e-4)\n",
        "    ap.add_argument(\"--weight_decay\", type=float, default=0.05)\n",
        "    ap.add_argument(\"--label_smoothing\", type=float, default=0.1)\n",
        "    ap.add_argument(\"--num_workers\", type=int, default=4)\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--patience\", type=int, default=4, help=\"Early stopping patience (by val_acc plateau)\")\n",
        "    ap.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    cfg = TrainConfig(\n",
        "        data_dir=args.data_dir,\n",
        "        output_dir=args.output_dir,\n",
        "        model_name=args.model_name,\n",
        "        img_size=args.img_size,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs_head=args.epochs_head,\n",
        "        epochs_ft=args.epochs_ft,\n",
        "        unfreeze_blocks=args.unfreeze_blocks,\n",
        "        lr_head=args.lr_head,\n",
        "        lr_ft=args.lr_ft,\n",
        "        weight_decay=args.weight_decay,\n",
        "        label_smoothing=args.label_smoothing,\n",
        "        num_workers=args.num_workers,\n",
        "        seed=args.seed,\n",
        "        patience=args.patience,\n",
        "        grad_clip=args.grad_clip,\n",
        "    )\n",
        "\n",
        "    seed_everything(cfg.seed)\n",
        "    device = get_device()\n",
        "\n",
        "    out_dir = Path(cfg.output_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Data\n",
        "    train_tf, val_tf = build_transforms(cfg.img_size)\n",
        "    train_dir = Path(cfg.data_dir) / \"train\"\n",
        "    val_dir = Path(cfg.data_dir) / \"val\"\n",
        "\n",
        "    if not train_dir.exists() or not val_dir.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Expected {train_dir} and {val_dir} to exist. \"\n",
        "            f\"If you only have class folders, run split_dataset.py first.\"\n",
        "        )\n",
        "\n",
        "    ds_train = datasets.ImageFolder(str(train_dir), transform=train_tf)\n",
        "    ds_val = datasets.ImageFolder(str(val_dir), transform=val_tf)\n",
        "\n",
        "    if ds_train.classes != ds_val.classes:\n",
        "        raise ValueError(f\"Class mismatch. train classes={ds_train.classes}, val classes={ds_val.classes}\")\n",
        "\n",
        "    num_classes = len(ds_train.classes)\n",
        "    class_to_idx = ds_train.class_to_idx\n",
        "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "    # Class weights\n",
        "    class_weights = compute_class_weights(ds_train.targets, num_classes=num_classes).to(device)\n",
        "\n",
        "    # Loss\n",
        "    try:\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=cfg.label_smoothing)\n",
        "    except TypeError:\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Loaders\n",
        "    pin_memory = (device.type == \"cuda\")\n",
        "    dl_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    dl_val = DataLoader(\n",
        "        ds_val,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # Model\n",
        "    model = timm.create_model(cfg.model_name, pretrained=True, num_classes=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    # Save config immediately\n",
        "    with open(out_dir / \"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(asdict(cfg), f, indent=2)\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Classes: {ds_train.classes}\")\n",
        "    print(f\"Class weights: {class_weights.detach().cpu().tolist()}\")\n",
        "\n",
        "    best_acc = -1.0\n",
        "    best_epoch = -1\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    def save_ckpt(path: Path, epoch: int, val_acc: float):\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_name\": cfg.model_name,\n",
        "                \"img_size\": cfg.img_size,\n",
        "                \"mean\": IMAGENET_MEAN,\n",
        "                \"std\": IMAGENET_STD,\n",
        "                \"classes\": ds_train.classes,\n",
        "                \"class_to_idx\": class_to_idx,\n",
        "                \"idx_to_class\": idx_to_class,\n",
        "                \"epoch\": epoch,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"state_dict\": model.state_dict(),\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "\n",
        "    # Phase 1: head-only\n",
        "    print(\"\\n=== Phase 1: Train head (freeze backbone) ===\")\n",
        "    freeze_all_but_head(model)\n",
        "    optimizer = make_optimizer(model, lr=cfg.lr_head, weight_decay=cfg.weight_decay, head_lr_mult=1.0)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, cfg.epochs_head))\n",
        "\n",
        "    for epoch in range(cfg.epochs_head):\n",
        "        train_loss = train_one_epoch(model, dl_train, optimizer, criterion, device, cfg.grad_clip)\n",
        "        metrics = evaluate(model, dl_val, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        val_acc = metrics[\"val_acc\"]\n",
        "        print(\n",
        "            f\"[Head] Epoch {epoch+1}/{cfg.epochs_head} | \"\n",
        "            f\"train_loss={train_loss:.4f} | val_acc={val_acc:.4f} | macro_f1={metrics['macro_f1']:.4f}\"\n",
        "        )\n",
        "\n",
        "        save_ckpt(out_dir / \"last.pt\", epoch=epoch, val_acc=val_acc)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_epoch = epoch\n",
        "            epochs_no_improve = 0\n",
        "            save_ckpt(out_dir / \"best.pt\", epoch=epoch, val_acc=val_acc)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "    # Phase 2: partial fine-tune\n",
        "    print(\"\\n=== Phase 2: Fine-tune last transformer blocks + head ===\")\n",
        "    unfreeze_last_blocks(model, unfreeze_blocks=cfg.unfreeze_blocks)\n",
        "    optimizer = make_optimizer(model, lr=cfg.lr_ft, weight_decay=cfg.weight_decay, head_lr_mult=5.0)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, cfg.epochs_ft))\n",
        "\n",
        "    for epoch in range(cfg.epochs_ft):\n",
        "        train_loss = train_one_epoch(model, dl_train, optimizer, criterion, device, cfg.grad_clip)\n",
        "        metrics = evaluate(model, dl_val, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        global_epoch = cfg.epochs_head + epoch\n",
        "        val_acc = metrics[\"val_acc\"]\n",
        "        print(\n",
        "            f\"[FT]   Epoch {epoch+1}/{cfg.epochs_ft} (global {global_epoch}) | \"\n",
        "            f\"train_loss={train_loss:.4f} | val_acc={val_acc:.4f} | macro_f1={metrics['macro_f1']:.4f}\"\n",
        "        )\n",
        "\n",
        "        save_ckpt(out_dir / \"last.pt\", epoch=global_epoch, val_acc=val_acc)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_epoch = global_epoch\n",
        "            epochs_no_improve = 0\n",
        "            save_ckpt(out_dir / \"best.pt\", epoch=global_epoch, val_acc=val_acc)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= cfg.patience:\n",
        "            print(f\"\\nEarly stopping: no improvement in {cfg.patience} evals.\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nBest val_acc={best_acc:.4f} at epoch={best_epoch}\")\n",
        "    print(f\"Saved checkpoints in: {out_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sCYB7jRptc_V"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L9C2xFdAtk29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e205a8-6197-4951-b3e6-ef88d5fd0c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.safetensors: 100% 88.2M/88.2M [00:01<00:00, 69.2MB/s]\n",
            "Device: cuda\n",
            "Classes: ['grade_A', 'grade_B', 'grade_C']\n",
            "Class weights: [1.622666597366333, 2.817129611968994, 0.49291208386421204]\n",
            "\n",
            "=== Phase 1: Train head (freeze backbone) ===\n",
            "/content/train_deit.py:197: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/content/train_deit.py:206: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "[Head] Epoch 1/4 | train_loss=0.8310 | val_acc=0.7986 | macro_f1=0.7275\n",
            "[Head] Epoch 2/4 | train_loss=0.6967 | val_acc=0.8141 | macro_f1=0.7367\n",
            "[Head] Epoch 3/4 | train_loss=0.6498 | val_acc=0.8569 | macro_f1=0.7809\n",
            "[Head] Epoch 4/4 | train_loss=0.6485 | val_acc=0.8655 | macro_f1=0.7914\n",
            "\n",
            "=== Phase 2: Fine-tune last transformer blocks + head ===\n",
            "[FT]   Epoch 1/10 (global 4) | train_loss=0.6933 | val_acc=0.9349 | macro_f1=0.8931\n",
            "[FT]   Epoch 2/10 (global 5) | train_loss=0.6244 | val_acc=0.9220 | macro_f1=0.8838\n",
            "[FT]   Epoch 3/10 (global 6) | train_loss=0.5836 | val_acc=0.8243 | macro_f1=0.8016\n",
            "[FT]   Epoch 4/10 (global 7) | train_loss=0.5537 | val_acc=0.7678 | macro_f1=0.7351\n",
            "[FT]   Epoch 5/10 (global 8) | train_loss=0.5354 | val_acc=0.8175 | macro_f1=0.8303\n",
            "\n",
            "Early stopping: no improvement in 4 evals.\n",
            "\n",
            "Best val_acc=0.9349 at epoch=4\n",
            "Saved checkpoints in: /content/outputs_deit\n"
          ]
        }
      ],
      "source": [
        "!python train_deit.py \\\n",
        "  --data_dir \"/content/dataset\" \\\n",
        "  --output_dir \"/content/outputs_deit\" \\\n",
        "  --model_name \"deit_small_patch16_224\" \\\n",
        "  --batch_size 32 \\\n",
        "  --epochs_head 4 \\\n",
        "  --epochs_ft 10 \\\n",
        "  --unfreeze_blocks 4 \\\n",
        "  --lr_head 1e-3 \\\n",
        "  --lr_ft 1e-4 \\\n",
        "  --num_workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fIclI6fDSA2l"
      },
      "outputs": [],
      "source": [
        "!find /content/drive -name \"best.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrPm6kduzbRV",
        "outputId": "0096a92d-4b7c-455e-8642-ef506a5f6999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /content/app.py\n",
            "\tzip warning: name not matched: /content/predict.py\n",
            "\tzip warning: name not matched: /content/outputs_deit2\n",
            "\tzip warning: name not matched: /content/cloudflared-linux-amd64.deb\n",
            "  adding: content/train_deit.py (deflated 71%)\n",
            "  adding: content/outputs_deit/ (stored 0%)\n",
            "  adding: content/outputs_deit/best.pt (deflated 7%)\n",
            "  adding: content/outputs_deit/config.json (deflated 45%)\n",
            "  adding: content/outputs_deit/last.pt (deflated 7%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r fairwool_project.zip \\\n",
        "/content/app.py \\\n",
        "/content/predict.py \\\n",
        "/content/train_deit.py \\\n",
        "/content/outputs_deit \\\n",
        "/content/outputs_deit2 \\\n",
        "/content/cloudflared-linux-amd64.deb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GftqV2yJXP2K",
        "outputId": "82d0beb9-9e2d-4111-ff4e-daa107e7c78d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8c4efb50-d9bc-4779-8f41-2f001860975e\", \"fairwool_project.zip\", 160733557)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"fairwool_project.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iAzIDTdM0B6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8698e1ad-0688-4de2-bb2a-3dbbebe63753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict.py\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def build_infer_transform(img_size: int, mean, std):\n",
        "    resize_size = int(round(img_size * 256 / 224))\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(resize_size),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--checkpoint\", type=str, required=True)\n",
        "    ap.add_argument(\"--image_path\", type=str, required=True)\n",
        "    ap.add_argument(\"--topk\", type=int, default=3)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    ckpt = torch.load(args.checkpoint, map_location=\"cpu\")\n",
        "\n",
        "    model_name = ckpt[\"model_name\"]\n",
        "    classes = ckpt[\"classes\"]\n",
        "    img_size = ckpt[\"img_size\"]\n",
        "    mean = ckpt[\"mean\"]\n",
        "    std = ckpt[\"std\"]\n",
        "\n",
        "    device = get_device()\n",
        "\n",
        "    model = timm.create_model(model_name, pretrained=False, num_classes=len(classes))\n",
        "    model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    tf = build_infer_transform(img_size, mean, std)\n",
        "\n",
        "    img = Image.open(args.image_path).convert(\"RGB\")\n",
        "    x = tf(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = F.softmax(logits, dim=1).squeeze(0).detach().cpu()\n",
        "\n",
        "    topk = min(args.topk, len(classes))\n",
        "    values, indices = torch.topk(probs, k=topk)\n",
        "\n",
        "    pred_idx = int(torch.argmax(probs).item())\n",
        "    pred_class = classes[pred_idx]\n",
        "    pred_prob = float(probs[pred_idx].item())\n",
        "\n",
        "    print(f\"Prediction: {pred_class} (p={pred_prob:.4f})\")\n",
        "    print(\"Top-k:\")\n",
        "    for v, i in zip(values.tolist(), indices.tolist()):\n",
        "        print(f\"  {classes[i]}: {v:.4f}\")\n",
        "\n",
        "    # Optional: print full distribution\n",
        "    print(\"\\nFull probabilities:\")\n",
        "    for cls, p in zip(classes, probs.tolist()):\n",
        "        print(f\"  {cls}: {p:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VHldOfewtQLa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VRmueVt-0XaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf04f5d4-3a79-4b80-9e0e-adbec4aa8890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMAGE: /content/dataset/val/grade_A/79d04b31eca473391201305076.jpg\n",
            "Prediction: grade_A (p=0.7413)\n",
            "Top-k:\n",
            "  grade_A: 0.7413\n",
            "  grade_B: 0.1464\n",
            "  grade_C: 0.1123\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.7413\n",
            "  grade_B: 0.1464\n",
            "  grade_C: 0.1123\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_A/d32a51126513ed621235157601.jpg\n",
            "Prediction: grade_A (p=0.6185)\n",
            "Top-k:\n",
            "  grade_A: 0.6185\n",
            "  grade_C: 0.2373\n",
            "  grade_B: 0.1442\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.6185\n",
            "  grade_B: 0.1442\n",
            "  grade_C: 0.2373\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_C/line_2018-10-10 11_47_45.349139.jpg\n",
            "Prediction: grade_C (p=0.6620)\n",
            "Top-k:\n",
            "  grade_C: 0.6620\n",
            "  grade_A: 0.1908\n",
            "  grade_B: 0.1472\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.1908\n",
            "  grade_B: 0.1472\n",
            "  grade_C: 0.6620\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_C/371.jpg\n",
            "Prediction: grade_C (p=0.7361)\n",
            "Top-k:\n",
            "  grade_C: 0.7361\n",
            "  grade_B: 0.1388\n",
            "  grade_A: 0.1252\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.1252\n",
            "  grade_B: 0.1388\n",
            "  grade_C: 0.7361\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_A/0017_000_00.png\n",
            "Prediction: grade_A (p=0.9716)\n",
            "Top-k:\n",
            "  grade_A: 0.9716\n",
            "  grade_B: 0.0202\n",
            "  grade_C: 0.0082\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.9716\n",
            "  grade_B: 0.0202\n",
            "  grade_C: 0.0082\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_B/OCTImage 054010_07172020.png\n",
            "Prediction: grade_B (p=0.9626)\n",
            "Top-k:\n",
            "  grade_B: 0.9626\n",
            "  grade_A: 0.0357\n",
            "  grade_C: 0.0018\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.0357\n",
            "  grade_B: 0.9626\n",
            "  grade_C: 0.0018\n"
          ]
        }
      ],
      "source": [
        "import random, glob\n",
        "\n",
        "images = []\n",
        "for c in [\"grade_A\",\"grade_B\",\"grade_C\"]:\n",
        "    images += glob.glob(f\"/content/dataset/val/{c}/*\")\n",
        "\n",
        "for img in random.sample(images, 6):\n",
        "    print(\"\\nIMAGE:\", img)\n",
        "    !python predict.py --checkpoint \"/content/outputs_deit/best.pt\" --image_path \"$img\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cP4Gw6k1Fl0v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AtJhYtq12m9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee52798-db6e-46dc-81f5-f5ac56dc7525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0001_000_01.png\n",
            "0001_000_02.png\n",
            "0001_000_03.png\n",
            "0001_000_05.png\n",
            "0002_000_00.png\n",
            "0002_000_02.png\n",
            "0002_000_03.png\n",
            "0002_000_04.png\n",
            "0002_000_05.png\n",
            "0002_000_06.png\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/dataset/val/grade_A\" | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZY6PG43K2xiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151509f0-83c4-414b-d4b9-136d2a9da6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/predict.py\", line 79, in <module>\n",
            "    main()\n",
            "  File \"/content/predict.py\", line 53, in main\n",
            "    img = Image.open(args.image_path).convert(\"RGB\")\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/Image.py\", line 3513, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/dataset/val/grade_A//content/dataset/val/grade_A//content/dataset/val/grade_A/00bd30945ae5fb701316505594.jpg'\n"
          ]
        }
      ],
      "source": [
        "!python predict.py \\\n",
        "  --checkpoint \"/content/outputs_deit/best.pt\" \\\n",
        "  --image_path \"/content/dataset/val/grade_A//content/dataset/val/grade_A//content/dataset/val/grade_A/00bd30945ae5fb701316505594.jpg\" \\\n",
        "  --topk 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "roafqeS-4KbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39357ae2-27ea-4f27-febc-0fefa9e0019c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset/val/grade_A/00bd30945ae5fb701316505594.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/dataset/val/grade_A/00bd30945ae5fb701316505594.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yBWKlcuJ5Rgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335aebb4-e6e2-43e2-ad9e-a1e414f6ba24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: grade_A (p=0.6354)\n",
            "Top-k:\n",
            "  grade_A: 0.6354\n",
            "  grade_B: 0.2133\n",
            "  grade_C: 0.1513\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.6354\n",
            "  grade_B: 0.2133\n",
            "  grade_C: 0.1513\n"
          ]
        }
      ],
      "source": [
        "!python predict.py \\\n",
        "  --checkpoint \"/content/outputs_deit/best.pt\" \\\n",
        "  --image_path \"/content/dataset/val/grade_A/00bd30945ae5fb701316505594.jpg\" \\\n",
        "  --topk 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlDQnmaVFnGK",
        "outputId": "e2a70ab0-4e21-485e-8dd1-278c874119e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMAGE: /content/dataset/val/grade_A/b774903aca7e08be1548164385.jpg\n",
            "Prediction: grade_A (p=0.6697)\n",
            "Top-k:\n",
            "  grade_A: 0.6697\n",
            "  grade_B: 0.1767\n",
            "  grade_C: 0.1536\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.6697\n",
            "  grade_B: 0.1767\n",
            "  grade_C: 0.1536\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_A/0001_000_01.png\n",
            "Prediction: grade_A (p=0.9429)\n",
            "Top-k:\n",
            "  grade_A: 0.9429\n",
            "  grade_B: 0.0503\n",
            "  grade_C: 0.0068\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.9429\n",
            "  grade_B: 0.0503\n",
            "  grade_C: 0.0068\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_C/hole_2018-10-11 13_59_17.317728.jpg\n",
            "Prediction: grade_C (p=0.7580)\n",
            "Top-k:\n",
            "  grade_C: 0.7580\n",
            "  grade_A: 0.1481\n",
            "  grade_B: 0.0939\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.1481\n",
            "  grade_B: 0.0939\n",
            "  grade_C: 0.7580\n",
            "\n",
            "IMAGE: /content/dataset/val/grade_A/54.jpg\n",
            "Prediction: grade_A (p=0.4643)\n",
            "Top-k:\n",
            "  grade_A: 0.4643\n",
            "  grade_C: 0.3934\n",
            "  grade_B: 0.1422\n",
            "\n",
            "Full probabilities:\n",
            "  grade_A: 0.4643\n",
            "  grade_B: 0.1422\n",
            "  grade_C: 0.3934\n"
          ]
        }
      ],
      "source": [
        "import random, glob\n",
        "\n",
        "images = []\n",
        "for c in [\"grade_A\",\"grade_B\",\"grade_C\"]:\n",
        "    images += glob.glob(f\"/content/dataset/val/{c}/*\")\n",
        "\n",
        "for img in random.sample(images, 4):\n",
        "    print(\"\\nIMAGE:\", img)\n",
        "    !python predict.py --checkpoint \"/content/outputs_deit2/best.pt\" --image_path \"$img\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XlTQv0I6YcP",
        "outputId": "f8e73d01-d287-4514-d7c9-1fa04e9a343d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "\n",
        "# ---------------- THEME ----------------\n",
        "st.set_page_config(\n",
        "    page_title=\"ReWoolution AI\",\n",
        "    page_icon=\"üêë\",\n",
        "    layout=\"centered\"\n",
        ")\n",
        "\n",
        "# Pastoral theme colors\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "body {\n",
        "    background-color: #f5f1ed;\n",
        "}\n",
        ".main-title {\n",
        "    font-size: 36px;\n",
        "    font-weight: 700;\n",
        "    color: #404954;\n",
        "}\n",
        ".subtitle {\n",
        "    font-size: 18px;\n",
        "    color: #70635B;\n",
        "}\n",
        ".card {\n",
        "    padding: 20px;\n",
        "    border-radius: 12px;\n",
        "    background-color: white;\n",
        "    box-shadow: 0px 4px 12px rgba(0,0,0,0.08);\n",
        "}\n",
        ".gradeA {color: #2E7D32; font-weight: bold;}\n",
        ".gradeB {color: #F9A825; font-weight: bold;}\n",
        ".gradeC {color: #C62828; font-weight: bold;}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ---------- Load model ----------\n",
        "ckpt = torch.load(\"outputs_deit2/best.pt\", map_location=\"cpu\")\n",
        "classes = ckpt[\"classes\"]\n",
        "img_size = ckpt[\"img_size\"]\n",
        "mean, std = ckpt[\"mean\"], ckpt[\"std\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = timm.create_model(ckpt[\"model_name\"], pretrained=False, num_classes=len(classes))\n",
        "model.load_state_dict(ckpt[\"state_dict\"])\n",
        "model.to(device).eval()\n",
        "\n",
        "# ---------- Transform ----------\n",
        "tf = transforms.Compose([\n",
        "    transforms.Resize(int(round(img_size * 256 / 224))),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# ---------- UI ----------\n",
        "st.markdown('<div class=\"main-title\">üêë FairWool AI</div>', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"subtitle\">AI-powered grading for sustainable wool ecosystems</div>', unsafe_allow_html=True)\n",
        "st.markdown(\"---\")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload wool image\", type=[\"jpg\",\"png\",\"jpeg\"])\n",
        "\n",
        "if uploaded:\n",
        "    img = Image.open(uploaded).convert(\"RGB\")\n",
        "    st.image(img, caption=\"Input Image\", use_container_width=True)\n",
        "\n",
        "    x = tf(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        probs = F.softmax(model(x), dim=1).squeeze(0).cpu()\n",
        "\n",
        "    pred_idx = int(torch.argmax(probs))\n",
        "    grade = classes[pred_idx]\n",
        "    conf = float(probs[pred_idx])\n",
        "\n",
        "    # Confidence label\n",
        "    if conf > 0.75:\n",
        "        conf_label = \"HIGH\"\n",
        "    elif conf > 0.55:\n",
        "        conf_label = \"MEDIUM\"\n",
        "    else:\n",
        "        conf_label = \"LOW ‚Äî Recommend manual review\"\n",
        "\n",
        "    # Use case mapping\n",
        "    if grade == \"grade_A\":\n",
        "        use_case = \"Premium fiber ‚Äì Textile / Fabric\"\n",
        "        grade_color = \"gradeA\"\n",
        "    elif grade == \"grade_B\":\n",
        "        use_case = \"Medium fiber ‚Äì Insulation / Craft\"\n",
        "        grade_color = \"gradeB\"\n",
        "    else:\n",
        "        use_case = \"Coarse fiber ‚Äì Industrial / Low-grade\"\n",
        "        grade_color = \"gradeC\"\n",
        "\n",
        "    st.markdown('<div class=\"card\">', unsafe_allow_html=True)\n",
        "    st.markdown(f\"### Predicted Grade: <span class='{grade_color}'>{grade}</span>\", unsafe_allow_html=True)\n",
        "    st.write(f\"Confidence: **{conf:.2f} ({conf_label})**\")\n",
        "    st.write(f\"Best Use: **{use_case}**\")\n",
        "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### üåç Social Impact\")\n",
        "    st.write(\"‚úî Helps pastoralists get fair grading\")\n",
        "    st.write(\"‚úî Reduces wool waste\")\n",
        "    st.write(\"‚úî Supports women cooperatives\")\n",
        "\n",
        "\n",
        "    st.markdown(\n",
        "    \"\"\"\n",
        "    ### üåæ Our Vision\n",
        "    > **‚ÄúToday, shepherds don‚Äôt know what their wool is worth.\n",
        "    > FairWool AI gives grading power and price intelligence directly in their hands ‚Äî\n",
        "    > turning waste into dignity.‚Äù**\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcnytAW37_1U",
        "outputId": "54e3db53-eb54-4507-8ded-9512d220262f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSelecting previously unselected package cloudflared.\n",
            "(Reading database ... 121852 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2026.2.0) ...\n",
            "Setting up cloudflared (2026.2.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[90m2026-02-21T21:23:59Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-02-21T21:23:59Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.186.4.241:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m |  https://parents-edwards-habitat-meet.trycloudflare.com                                    |\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Version 2026.2.0 (Checksum 176746db3be7dc7bd48f3dd287c8930a4645ebb6e6700f883fddda5a4c307c16)\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.13, GoArch: amd64\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 4391ff07-3dfb-48ba-87d4-0edb9101bb47\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.63\n",
            "2026/02/21 21:24:02 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-02-21T21:24:02Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m031a4015-666f-49de-9a84-5c08a532bd0b \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.63 \u001b[36mlocation=\u001b[0mlax06 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.63\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.63\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.63\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.63\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2026-02-21T21:24:47Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ],
      "source": [
        "!pkill -f streamlit || true\n",
        "!pip -q install streamlit timm torchvision\n",
        "\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "!streamlit run app.py --server.port 8501 --server.headless true --server.enableCORS false --server.enableXsrfProtection false & \\\n",
        "cloudflared tunnel --url http://localhost:8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir -p \"/content/drive/MyDrive/github_projects/my_ai_project\""
      ],
      "metadata": {
        "id": "KK3T4D8nF0i-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp \"/content/Untitled0.ipynb\" \"/content/drive/MyDrive/github_projects/my_ai_project/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EaSKvE2F9oh",
        "outputId": "997b0592-d4ae-44b5-ad25-97a86dae1b6b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/Untitled0.ipynb': No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}